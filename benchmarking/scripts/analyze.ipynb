{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze e2e latencies for the single_client_e2e experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistic/metric, compute e2e (us), confirm e2e (us), consume e2e (us), total e2e (us)\n",
      "mean, 1807.20, 2633.06, 787.75, 2651.66\n",
      "std, 567.27, 542.00, 261.30, 526.72\n",
      "p50, 1855.00, 2815.00, 739.00, 2815.00\n",
      "p99, 2445.00, 3164.00, 920.00, 3164.00\n",
      "average batch size, average batching latencies\n",
      "1.58, 30.41\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "computee2e, confirme2e = [], []\n",
    "consumee2e = []\n",
    "avg_batching_latencies = []\n",
    "avg_batch_size = []\n",
    "with open(\"../logs/client_node7.log\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if \"latencies: \" in line:\n",
    "            consume, confirm, compute = line.strip().split('latencies: ')[-1].split(',')[-3:]\n",
    "            consumee2e.append(int(consume))\n",
    "            confirme2e.append(int(confirm))\n",
    "            computee2e.append(int(compute))\n",
    "        if \"average batch size: \" in line:\n",
    "            avg_batch_size.append(float(line.strip().split()[-1]))\n",
    "        if \"average batching latency: \" in line:\n",
    "            avg_batching_latencies.append(float(line.strip().split()[-1]))\n",
    "\n",
    "computee2e = np.array(computee2e, dtype=int)\n",
    "confirme2e = np.array(confirme2e, dtype=int)\n",
    "consumee2e = np.array(consumee2e, dtype=int)\n",
    "\n",
    "temp = np.maximum(computee2e, confirme2e)\n",
    "\n",
    "print(\"statistic/metric, compute e2e (us), confirm e2e (us), consume e2e (us), total e2e (us)\")\n",
    "print(f\"mean, {np.mean(computee2e):.2f}, {np.mean(confirme2e):.2f}, {np.mean(consumee2e):.2f}, {np.mean(temp):.2f}\")\n",
    "print(f\"std, {np.std(computee2e):.2f}, {np.std(confirme2e):.2f}, {np.std(consumee2e):.2f}, {np.std(temp):.2f}\")\n",
    "print(f\"p50, {np.percentile(computee2e, 50):.2f}, {np.percentile(confirme2e, 50):.2f}, {np.percentile(consumee2e, 50):.2f}, {np.percentile(temp, 50):.2f}\")\n",
    "print(f\"p99, {np.percentile(computee2e, 90):.2f}, {np.percentile(confirme2e, 90):.2f}, {np.percentile(consumee2e, 90):.2f}, {np.percentile(temp, 90):.2f}\")\n",
    "\n",
    "print(\"average batch size, average batching latencies\")\n",
    "print(f\"{avg_batch_size[-1]:.2f}, {avg_batching_latencies[-1]:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
