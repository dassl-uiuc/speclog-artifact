{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze e2e latencies for the single_client_e2e experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def get_append_metrics(path):\n",
    "    file_pattern = path + \"append_metrics*.csv\"\n",
    "\n",
    "    total_throughput = 0\n",
    "    latency_values = []\n",
    "\n",
    "    for file in glob.glob(file_pattern):\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()[1:]\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                gsn, latency, throughput = int(parts[0]), float(parts[1]), float(parts[2])\n",
    "                latency_values.append(latency)\n",
    "            \n",
    "            total_throughput += throughput\n",
    "\n",
    "    latency_array = np.array(latency_values)\n",
    "\n",
    "    mean_latency = np.mean(latency_array)\n",
    "    p50_latency = np.percentile(latency_array, 50)\n",
    "    p99_latency = np.percentile(latency_array, 99)\n",
    "\n",
    "    print(\"results for computation time \" + path.split(\"_\")[-1].split(\"/\")[0] + \" us\")\n",
    "    print(\"statistic/metric, latency (us)\")\n",
    "    print(f\"mean, {mean_latency:.2f}\")\n",
    "    print(f\"p50, {p50_latency:.2f}\")\n",
    "    print(f\"p99, {p99_latency:.2f}\")\n",
    "    print(f\"total throughput, {total_throughput:.2f}\")\n",
    "\n",
    "    return int(path.split(\"_\")[-1].split(\"/\")[0]), mean_latency, total_throughput \n",
    "\n",
    "def get_e2e_metrics(path):\n",
    "    file_pattern = path + \"e2e_metrics*.csv\"\n",
    "\n",
    "    delivery_latency_values = []\n",
    "    compute_latency_values = []\n",
    "    confirmation_latency_values = []\n",
    "    e2e_latency_values = []\n",
    "    queuing_delay_values = []\n",
    "\n",
    "    for file in glob.glob(file_pattern):\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()[1:]\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                delivery, confirm, compute, e2e, queuing_delay = float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4]), float(parts[5])\n",
    "                delivery_latency_values.append(delivery)\n",
    "                compute_latency_values.append(compute)\n",
    "                confirmation_latency_values.append(confirm)\n",
    "                e2e_latency_values.append(e2e)\n",
    "                queuing_delay_values.append(queuing_delay)\n",
    "\n",
    "    \n",
    "    delivery_latency_array = np.array(delivery_latency_values)\n",
    "    compute_latency_array = np.array(compute_latency_values)\n",
    "    confirmation_latency_array = np.array(confirmation_latency_values)\n",
    "    e2e_latency_array = np.array(e2e_latency_values)\n",
    "    queuing_delay_array = np.array([x for x in queuing_delay_values if x > 0])\n",
    "\n",
    "    print(\"statistic/metric, delivery latency (us), compute latency (us), confirm latency (us), e2e latency (us), queuing delay (us)\")\n",
    "    print(f\"mean, {np.mean(delivery_latency_array):.2f}, {np.mean(compute_latency_array):.2f}, {np.mean(confirmation_latency_array):.2f}, {np.mean(e2e_latency_array):.2f}, {np.mean(queuing_delay_array):.2f}\")\n",
    "    print(f\"std, {np.std(delivery_latency_array):.2f}, {np.std(compute_latency_array):.2f}, {np.std(confirmation_latency_array):.2f}, {np.std(e2e_latency_array):.2f}, {np.std(queuing_delay_array):.2f}\")\n",
    "    print(f\"p50, {np.percentile(delivery_latency_array, 50):.2f}, {np.percentile(compute_latency_array, 50):.2f}, {np.percentile(confirmation_latency_array, 50):.2f}, {np.percentile(e2e_latency_array, 50):.2f}, {np.percentile(queuing_delay_array, 50):.2f}\")\n",
    "    print(f\"p99, {np.percentile(delivery_latency_array, 99):.2f}, {np.percentile(compute_latency_array, 99):.2f}, {np.percentile(confirmation_latency_array, 99):.2f}, {np.percentile(e2e_latency_array, 99):.2f}, {np.percentile(queuing_delay_array, 99):.2f}\")\n",
    "\n",
    "    return int(path.split(\"_\")[-1].split(\"/\")[0]), np.mean(delivery_latency_array), np.mean(compute_latency_array), np.mean(confirmation_latency_array), np.mean(e2e_latency_array), np.mean(queuing_delay_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../results/*\"\n",
    "\n",
    "for dir in glob.glob(path + \"*/\"):\n",
    "    get_append_metrics(dir)\n",
    "    get_e2e_metrics(dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Input data\n",
    "data = \"\"\"\n",
    "PASTE DATA FROM OUTPUT ABOVE\n",
    "\"\"\"\n",
    "\n",
    "# Split data into lines\n",
    "lines = data.splitlines()\n",
    "\n",
    "# Parsing logic\n",
    "results = []\n",
    "current_time = None\n",
    "append_latency = None\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    line = line.strip()\n",
    "    # Detect computation time\n",
    "    if line.startswith(\"results for computation time\"):\n",
    "        match = re.search(r\"computation time (\\d+) us\", line)\n",
    "        if match:\n",
    "            current_time = int(match.group(1))\n",
    "    # Extract append latency (first \"latency (us)\" mean value)\n",
    "    elif \"statistic/metric, latency (us)\" in line:\n",
    "        append_line = lines[i + 1]  # Look at the next line\n",
    "        append_match = re.match(r\"mean,\\s*([\\d.]+)\", append_line)\n",
    "        if append_match:\n",
    "            append_latency = float(append_match.group(1))\n",
    "    # Extract other latencies (mean values)\n",
    "    elif line.startswith(\"mean,\") and current_time is not None:\n",
    "        parts = line.split(\",\")\n",
    "        if len(parts) == 6:  # Ensure correct format\n",
    "            delivery_latency, compute_latency, confirm_latency, e2e_latency, queueing_delay = map(float, parts[1:])\n",
    "            results.append({\n",
    "                \"computation_time\": current_time,\n",
    "                \"append_latency\": append_latency,\n",
    "                \"delivery_latency\": delivery_latency,\n",
    "                \"compute_latency\": compute_latency,\n",
    "                \"confirm_latency\": confirm_latency,\n",
    "                \"e2e_latency\": e2e_latency,\n",
    "                \"queueing_delay\": queueing_delay,\n",
    "            })\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.to_csv(\"output.csv\", index=False)\n",
    "\n",
    "# Print the CSV content\n",
    "print(df.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze order server log from the reconfiguration experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# File path\n",
    "log_file = \"../results/reconfig_1000/order-0.log\"\n",
    "\n",
    "# Regex patterns\n",
    "tput_pattern = r\"\\[real-time tput\\]: (\\d+) ops/sec\"\n",
    "timestamp_pattern = r\"(\\d{2}:\\d{2}:\\d{2}\\.\\d{6})\"\n",
    "\n",
    "# Data storage\n",
    "timestamps = []\n",
    "tput_values = []\n",
    "shard_join_request = []\n",
    "shard_notified_to_be_added = []\n",
    "shard_notified_to_be_removed = []\n",
    "first_cut_committed = []\n",
    "last_cut_committed = []\n",
    "shard_leave_request = []\n",
    "\n",
    "# Parse the log file\n",
    "with open(log_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        # Extract real-time throughput\n",
    "        tput_match = re.search(tput_pattern, line)\n",
    "        timestamp_match = re.search(timestamp_pattern, line)\n",
    "        \n",
    "        if tput_match and timestamp_match:\n",
    "            tput_values.append(int(tput_match.group(1)))\n",
    "            timestamps.append(datetime.strptime(timestamp_match.group(1), \"%H:%M:%S.%f\"))\n",
    "        \n",
    "        # Extract events for annotation\n",
    "        if \"Shard 1 to be added in next avl window\" in line:\n",
    "            shard_join_request.append(timestamp_match.group(1))\n",
    "\n",
    "        if shard_notified_to_be_added == [] and \"shardQuotas:<key:2\" in line:\n",
    "            shard_notified_to_be_added.append(timestamp_match.group(1))\n",
    "        \n",
    "        if first_cut_committed == [] and \"cut:<key:2\" in line:\n",
    "            first_cut_committed.append(timestamp_match.group(1))\n",
    "        \n",
    "        if last_cut_committed == [] and \"Replica 2 finalized\" in line:\n",
    "            last_cut_committed.append(timestamp_match.group(1))\n",
    "\n",
    "        if \"Shard 1 to be finalized in next avl window\" in line:\n",
    "            shard_leave_request.append(timestamp_match.group(1))\n",
    "\n",
    "        if \"Incrementing view ID because shardFinalized: true\" in line:\n",
    "            shard_notified_to_be_removed.append(timestamp_match.group(1))\n",
    "\n",
    "\n",
    "# Convert timestamps to seconds since the start\n",
    "start_time = timestamps[0]\n",
    "time_in_seconds = [(ts - start_time).total_seconds() for ts in timestamps]\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_in_seconds, tput_values, label=\"Throughput (ops/sec)\", color=\"blue\")\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.ylabel(\"Throughput (ops/sec)\")\n",
    "plt.title(\"Real-time Throughput vs Time\")\n",
    "plt.grid()\n",
    "\n",
    "# Event times\n",
    "shard_join_request = (datetime.strptime(shard_join_request[0], \"%H:%M:%S.%f\") - start_time).total_seconds()\n",
    "shard_notified_to_be_added_time = (datetime.strptime(shard_notified_to_be_added[0], \"%H:%M:%S.%f\") - start_time).total_seconds()\n",
    "first_cut_committed_time = (datetime.strptime(first_cut_committed[0], \"%H:%M:%S.%f\") - start_time).total_seconds()\n",
    "shard_leave_request_time = (datetime.strptime(shard_leave_request[0], \"%H:%M:%S.%f\") - start_time).total_seconds()\n",
    "shard_notified_to_be_removed_time = (datetime.strptime(shard_notified_to_be_removed[0], \"%H:%M:%S.%f\") - start_time).total_seconds()\n",
    "last_cut_committed_time = (datetime.strptime(last_cut_committed[0], \"%H:%M:%S.%f\") - start_time).total_seconds()\n",
    "\n",
    "# Add vertical lines for events\n",
    "plt.axvline(shard_join_request, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"shard requests to join\")\n",
    "plt.axvline(shard_notified_to_be_added_time, color=\"green\", linestyle=\"--\", alpha=0.7, label=\"shard notified about addition window\")\n",
    "plt.axvline(first_cut_committed_time, color=\"purple\", linestyle=\"--\", alpha=0.7, label=\"first cut committed from new shard\")\n",
    "plt.axvline(shard_leave_request_time, color=\"orange\", linestyle=\"--\", alpha=0.7, label=\"shard requests to leave\")\n",
    "plt.axvline(shard_notified_to_be_removed_time, color=\"brown\", linestyle=\"--\", alpha=0.7, label=\"shard notified about removal window\")\n",
    "plt.axvline(last_cut_committed_time, color=\"black\", linestyle=\"--\", alpha=0.7, label=\"last cut committed from leaving shard\")\n",
    "\n",
    "# Zoom in to the relevant range (adjust as needed)\n",
    "# plt.xlim(22.8, 23.5)\n",
    "# plt.xlim(10, 65)\n",
    "# plt.xlim(51, 55)\n",
    "\n",
    "# Add legend outside the plot area\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), title=\"Events\")\n",
    "\n",
    "# Adjust layout to ensure the legend doesn't overlap with the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"adding_reconfig_throughput_annotated.png\", dpi=600)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze script for lagfix expt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "def find_nearest_multiple_of_3(number):\n",
    "    return math.ceil(number / 3) * 3\n",
    "    \n",
    "def process_logs(logfile1, logfile2, logfile3):\n",
    "    # Step 1: Extract the number from logfile1\n",
    "    with open(logfile1, 'r') as file1:\n",
    "        for line in file1:\n",
    "            match = re.search(r\"burst record with lsn (\\d+)\", line)\n",
    "            if match:\n",
    "                number = int(match.group(1))\n",
    "                y = find_nearest_multiple_of_3(number)\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(\"No 'burst record with lsn' found in logfile1\")\n",
    "    \n",
    "    # Step 2: Locate and fetch lines from logfile2\n",
    "    with open(logfile2, 'r') as file2:\n",
    "        lines = file2.readlines()\n",
    "    \n",
    "    target_index = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if f\"cut:{y} cut:0\" in line:\n",
    "            target_index = i\n",
    "            break\n",
    "    \n",
    "    if target_index is None:\n",
    "        raise ValueError(f\"No line containing 'cut:{y} cut:0' found in logfile2\")\n",
    "    \n",
    "    # Fetch 2000 lines before and after\n",
    "    start = max(0, target_index - 200)\n",
    "    end = min(len(lines), target_index + 200)\n",
    "    selected_lines = lines[start:end]\n",
    "    \n",
    "    # Step 3: Write to logfile3\n",
    "    with open(logfile3, 'w') as file3:\n",
    "        file3.writelines(selected_lines)\n",
    "\n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile1 = \"../results/lagfix_relaxed/data-0-0.log\"\n",
    "logfile2 = \"../results/lagfix_relaxed/order-0.log\"\n",
    "logfile3 = \"output.log\"\n",
    "\n",
    "y = process_logs(logfile1, logfile2, logfile3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_events(log_file, y):\n",
    "    shard_0_cuts = []\n",
    "    shard_1_cuts = []\n",
    "    \n",
    "    lagfix_detected = []\n",
    "    lagfix_notified = []\n",
    "    lagfix_resolved = []\n",
    "    burst_start = []\n",
    "\n",
    "    # Generic pattern for timestamps\n",
    "    timestamp_pattern = r\"(\\d{2}:\\d{2}:\\d{2}\\.\\d{6})\"\n",
    "\n",
    "    with open(log_file, 'r') as file:\n",
    "        for line in file:\n",
    "            # Check for shard 0 cuts\n",
    "            match_shard_0 = re.search(fr\"{timestamp_pattern} cut:(\\d+) cut:0\", line)\n",
    "            if match_shard_0:\n",
    "                timestamp, cut = match_shard_0.groups()\n",
    "                shard_0_cuts.append((timestamp, int(cut)))\n",
    "\n",
    "            # Check for shard 1 cuts\n",
    "            match_shard_1 = re.search(fr\"{timestamp_pattern} localReplicaID:1 cut:0 cut:(\\d+)\", line)\n",
    "            if match_shard_1:\n",
    "                timestamp, cut = match_shard_1.groups()\n",
    "                shard_1_cuts.append((timestamp, int(cut)))\n",
    "\n",
    "            if burst_start == [] and \"cut:\" + str(y) + \" cut:0\" in line:\n",
    "                burst_start.append(re.search(timestamp_pattern, line).group(1))\n",
    "\n",
    "            if burst_start != [] and lagfix_detected == [] and \"significant lag in cuts:\" in line:\n",
    "                lagfix_detected.append(re.search(timestamp_pattern, line).group(1))\n",
    "\n",
    "            if burst_start != [] and lagfix_notified == [] and \"adjustmentSignal:\" in line:\n",
    "                lagfix_notified.append(re.search(timestamp_pattern, line).group(1))\n",
    "            \n",
    "            if burst_start != [] and lagfix_resolved == [] and \"fixedLag:true\" in line:\n",
    "                lagfix_resolved.append(re.search(timestamp_pattern, line).group(1))\n",
    "\n",
    "\n",
    "    return shard_0_cuts, shard_1_cuts, lagfix_detected, lagfix_notified, lagfix_resolved, burst_start\n",
    "\n",
    "\n",
    "# Example usage\n",
    "log_file = \"output.log\"\n",
    "shard_0_cuts, shard_1_cuts, lagfix_detected, lagfix_notified, lagfix_resolved, burst_start = extract_events(log_file, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from matplotlib.ticker import MultipleLocator, MaxNLocator, FuncFormatter\n",
    "\n",
    "\n",
    "def parse_timestamp(timestamp):\n",
    "    return datetime.strptime(timestamp, \"%H:%M:%S.%f\")\n",
    "\n",
    "\n",
    "def plot_events_zoomed(\n",
    "    shard_0_cuts, shard_1_cuts, lagfix_detected, lagfix_notified, lagfix_resolved, burst_start, zoom_event=\"Lag Detected\", zoom_window_ms=100\n",
    "):\n",
    "    # Convert timestamps to datetime objects\n",
    "    shard_0_times = [parse_timestamp(t) for t, _ in shard_0_cuts]\n",
    "    shard_0_values = [1 for _ in shard_0_cuts]  # Level 1 for Shard 0\n",
    "\n",
    "    shard_1_times = [parse_timestamp(t) for t, _ in shard_1_cuts]\n",
    "    shard_1_values = [2 for _ in shard_1_cuts]  # Level 2 for Shard 1\n",
    "\n",
    "    # Parse event timestamps\n",
    "    event_times = {\n",
    "        \"Lag Detected\": [parse_timestamp(lagfix_detected[0])] if lagfix_detected else [],\n",
    "        \"Lag Notified\": [parse_timestamp(lagfix_notified[0])] if lagfix_notified else [],\n",
    "        \"Lag Resolved\": [parse_timestamp(lagfix_resolved[0])] if lagfix_resolved else [],\n",
    "        \"Burst Start\": [parse_timestamp(burst_start[0])] if burst_start else [],\n",
    "    }\n",
    "\n",
    "    # Event colors for differentiation\n",
    "    event_colors = {\n",
    "        \"Lag Detected\": \"red\",\n",
    "        \"Lag Notified\": \"purple\",\n",
    "        \"Lag Resolved\": \"green\",\n",
    "        \"Burst Start\": \"brown\",\n",
    "    }\n",
    "\n",
    "    # Find the minimum timestamp across all events and cuts\n",
    "    all_times = shard_0_times + shard_1_times\n",
    "    for times in event_times.values():\n",
    "        all_times.extend(times)\n",
    "\n",
    "    min_timestamp = min(all_times)\n",
    "\n",
    "    # Convert all timestamps to relative time (milliseconds since min_timestamp)\n",
    "    def to_relative_ms(time):\n",
    "        return (time - min_timestamp).total_seconds() * 1000\n",
    "\n",
    "    shard_0_times_rel = [to_relative_ms(t) for t in shard_0_times]\n",
    "    shard_1_times_rel = [to_relative_ms(t) for t in shard_1_times]\n",
    "\n",
    "    # Parse event times as relative times\n",
    "    event_times_rel = {\n",
    "        event_name: [to_relative_ms(t) for t in times]\n",
    "        for event_name, times in event_times.items()\n",
    "    }\n",
    "\n",
    "    # Determine the time range for zooming\n",
    "    if zoom_event not in event_times_rel or not event_times_rel[zoom_event]:\n",
    "        print(f\"No event found for {zoom_event}.\")\n",
    "        return\n",
    "\n",
    "    zoom_time = event_times_rel[zoom_event][0]\n",
    "    zoom_start = zoom_time - zoom_window_ms\n",
    "    zoom_end = zoom_time + zoom_window_ms\n",
    "\n",
    "    # Filter data for the zoomed time range\n",
    "    shard_0_times_zoomed = [t for t in shard_0_times_rel if zoom_start <= t <= zoom_end]\n",
    "    shard_1_times_zoomed = [t for t in shard_1_times_rel if zoom_start <= t <= zoom_end]\n",
    "\n",
    "    # Start plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot cut points for Shard 0 with stacking\n",
    "    ax.scatter(shard_0_times_zoomed, [1 + 0.05 * i for i in range(len(shard_0_times_zoomed))], label=\"Shard 0 Cuts\", color=\"blue\", marker=\"o\", s=40, alpha=0.6)\n",
    "\n",
    "    # Plot cut points for Shard 1 with stacking\n",
    "    ax.scatter(shard_1_times_zoomed, [2 + 0.05 * i for i in range(len(shard_1_times_zoomed))], label=\"Shard 1 Cuts\", color=\"orange\", marker=\"o\", s=40, alpha=0.6)\n",
    "\n",
    "    # Plot vertical lines for events within the zoom window\n",
    "    for event_name, times in event_times_rel.items():\n",
    "        for time in times:\n",
    "            if zoom_start <= time <= zoom_end:\n",
    "                ax.axvline(time, linestyle=\"--\", label=event_name, color=event_colors[event_name], alpha=0.7)\n",
    "\n",
    "    # Adjust x-axis for 1 ms ticks\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(1))  # 1 ms ticks\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x:.0f}\"))  # Show milliseconds as integers\n",
    "\n",
    "    # Adjust number of ticks on the x-axis\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=10, prune='both'))  # Use nbins to control tick density\n",
    "\n",
    "    # Set the x-limits to zoom in on the window\n",
    "    ax.set_xlim(zoom_start, zoom_end)\n",
    "\n",
    "    # Rotate x-axis labels for better readability\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Adjust y-axis\n",
    "    ax.set_yticks([1, 2])\n",
    "    ax.set_yticklabels([\"Shard 0\", \"Shard 1\"])\n",
    "\n",
    "    # Add labels and legend\n",
    "    ax.set_xlabel(\"Time (ms relative to min timestamp)\")\n",
    "    ax.set_ylabel(\"Shards\")\n",
    "    ax.set_title(f\"Zoomed View: {zoom_event} (±{zoom_window_ms} ms)\")\n",
    "    ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"Legend\", shadow=True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output.png\", dpi=600)\n",
    "\n",
    "\n",
    "# Example usage (assuming shard_0_cuts, shard_1_cuts, lagfix_detected, lagfix_notified, lagfix_resolved, burst_start are defined)\n",
    "plot_events_zoomed(shard_0_cuts, shard_1_cuts, lagfix_detected, lagfix_notified, lagfix_resolved, burst_start, zoom_event=\"Burst Start\", zoom_window_ms=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze script for the aggresive lagfix vs no lagfix latency plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def get_append_metrics_for_lagfix(path):\n",
    "    file_pattern = path + \"append_latency_timestamp_0.csv\"\n",
    "    latency_values = {}\n",
    "    gsns = []\n",
    "\n",
    "    for file in glob.glob(file_pattern):\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()[1:]  # Skip the header\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                gsn, timestamp, latency = int(parts[0]), parts[1], int(parts[2])\n",
    "                latency_values[gsn] = [latency, timestamp]\n",
    "\n",
    "    client_file = path + \"client_node13_0.log\"\n",
    "    with open(client_file, 'r') as f:\n",
    "        log_data = f.read()\n",
    "        for line in log_data.splitlines():\n",
    "            if \"gsn: \" in line:\n",
    "                try:\n",
    "                    gsn = int(line.split()[-1])  \n",
    "                    gsns.append(gsn)\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping invalid GSN: {line.split()[-1]}\")\n",
    "\n",
    "    gsns.sort()\n",
    "    return latency_values, gsns\n",
    "\n",
    "# CONFIGURE PATH HERE (without or aggressive)\n",
    "path = \"../results/lagfix_without/\"\n",
    "latency_values, gsns = get_append_metrics_for_lagfix(path)\n",
    "\n",
    "latency_array = np.array([latency_values[gsn][0] for gsn in latency_values])\n",
    "mean_latency = np.mean(latency_array)\n",
    "p99_latency = np.percentile(latency_array, 99)\n",
    "\n",
    "print(f\"Mean latency: {mean_latency:.2f} us\")\n",
    "print(f\"P99 latency: {p99_latency:.2f} us\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "latency_times = []\n",
    "latencies = []\n",
    "for gsn, (latency, timestamp) in latency_values.items():\n",
    "    latency_times.append(datetime.strptime(timestamp, \"%H:%M:%S.%f\"))\n",
    "    latencies.append(latency)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'time': latency_times,\n",
    "    'latency': latencies\n",
    "})\n",
    "\n",
    "df = df.sort_values(by='time')\n",
    "\n",
    "# CONFIGURE WINDOW SIZE HERE\n",
    "window_size = 10  # Set the window size for the moving average\n",
    "df['moving_avg'] = df['latency'].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "min_time = df['time'].min()\n",
    "df['relative_time_ms'] = (df['time'] - min_time).dt.total_seconds() * 1000\n",
    "\n",
    "# CONFIGURE ZOOM IN PERIOD HERE\n",
    "start_time = datetime.strptime(latency_values[gsns[0]][1], \"%H:%M:%S.%f\") - timedelta(milliseconds=100)  \n",
    "end_time = datetime.strptime(latency_values[gsns[-1]][1], \"%H:%M:%S.%f\") + timedelta(milliseconds=100)  \n",
    "\n",
    "df_zoomed = df[(df['time'] >= start_time) & (df['time'] <= end_time)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_zoomed['relative_time_ms'], df_zoomed['moving_avg'], label=f'Moving Average (window={window_size})', color='blue', linewidth=2)\n",
    "\n",
    "\n",
    "start_time_relative = (datetime.strptime(latency_values[gsns[0]][1], \"%H:%M:%S.%f\") - min_time).total_seconds() * 1000\n",
    "end_time_relative = (datetime.strptime(latency_values[gsns[-1]][1], \"%H:%M:%S.%f\") - min_time).total_seconds() * 1000\n",
    "plt.axvline(x=start_time_relative, color='green', linestyle=':', label=f'burst start time')\n",
    "plt.axvline(x=end_time_relative, color='red', linestyle=':', label=f'burst end time')\n",
    "\n",
    "\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Latency (us)')\n",
    "plt.title('Latency Over Time with Moving Average')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(ymin=0)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_latency_vs_gsn(latency_values, gsns):\n",
    "    sequence_numbers = list(latency_values.keys())\n",
    "    latencies = [latency_values[gsn][0] for gsn in sequence_numbers]\n",
    "\n",
    "    burst_start_gsn = gsns[0]\n",
    "    burst_end_gsn = gsns[-1]\n",
    "    \n",
    "    start_index = sequence_numbers.index(burst_start_gsn)\n",
    "    end_index = sequence_numbers.index(burst_end_gsn)\n",
    "    \n",
    "    # CONFIGURE WINDOW SIZE HERE\n",
    "    window_start = max(start_index - 50, 0)\n",
    "    window_end = min(end_index + 50, len(sequence_numbers) - 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(sequence_numbers[window_start:window_end+1], latencies[window_start:window_end+1], linestyle='-', label=\"Latency\")\n",
    "\n",
    "    plt.axvline(x=sequence_numbers[start_index], color='green', linestyle='--', label=\"Burst Start (GSN)\")\n",
    "    plt.axvline(x=sequence_numbers[end_index], color='purple', linestyle='--', label=\"Burst End (GSN)\")\n",
    "\n",
    "    plt.xlabel(\"GSN (Sequence Number)\")\n",
    "    plt.ylabel(\"Latency (us)\")\n",
    "    plt.title(\"Impact of bursts on latency\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"latency_vs_gsn.png\", dpi=600)\n",
    "\n",
    "plot_latency_vs_gsn(latency_values, gsns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failure Detection Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path='../results/1ms/append_bench_40/<hpnode*>_1m_4096_*.csv'\n",
    "\n",
    "all_metric : list[tuple[int, int, datetime]] = []\n",
    "for file in glob.glob(path):\n",
    "    print(f'processing {file}')\n",
    "    with open(file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            try:\n",
    "                all_metric.append((int(row.get('gsn')), int(row.get('latency(ns)')), datetime.strptime(row.get('runEndTime'), '%H:%M:%S.%f')))\n",
    "            except Exception as e:\n",
    "                print(f'Invalid value in row: {row}, {e}')\n",
    "print('sorting..')\n",
    "all_metric.sort(key=lambda x: x[0])\n",
    "all_time = [l[2] for l in all_metric]\n",
    "all_lat = [l[1] for l in all_metric]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path='../results/e2e_2000/e2e_metrics_*.csv'\n",
    "all_e2e_metric : list[tuple[int, int, datetime]] = []\n",
    "for file in glob.glob(path):\n",
    "    print(f'processing {file}')\n",
    "    with open(file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            try:\n",
    "                all_e2e_metric.append((int(row.get('gsn')), max(int(row.get('e2e latency (us)')), int(row.get('recompute latency (us)'))), datetime.strptime(row.get('start time'), '%H:%M:%S.%f')))\n",
    "            except Exception as e:\n",
    "                print(f'Invalid value in row: {row}, {e}')\n",
    "\n",
    "path='../results/e2e_2000/append_metrics_*.csv'\n",
    "all_append_metric : list[tuple[int, int]] = []\n",
    "for file in glob.glob(path):\n",
    "    print(f'processing {file}')\n",
    "    with open(file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            try:\n",
    "                all_append_metric.append((int(row.get('gsn')), int(row.get('latency (us)'))))\n",
    "            except Exception as e:\n",
    "                print(f'Invalid value in row: {row}, {e}')\n",
    "\n",
    "all_e2e_metric.sort(key=lambda x: x[0])\n",
    "all_append_metric.sort(key=lambda x: x[0])\n",
    "all_time = [l[2] for l in all_e2e_metric]\n",
    "all_e2e_lat = [l[1] for l in all_e2e_metric]\n",
    "all_append_lat = [l[1] for l in all_append_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame({\n",
    "    'e2e_latency': all_e2e_lat,\n",
    "    'app_latency': all_append_lat,\n",
    "    'time': all_time,\n",
    "})\n",
    "df = df.sort_values(by='time')\n",
    "\n",
    "zoom = (28, 29)\n",
    "\n",
    "start_time = all_time[0] + timedelta(seconds=zoom[0])\n",
    "end_time = all_time[0] + timedelta(seconds=zoom[1])\n",
    "\n",
    "df['e2e_mov_avg'] = df['e2e_latency'].rolling(window=10, min_periods=1).mean()/1e3\n",
    "df['app_mov_avg'] = df['app_latency'].rolling(window=10, min_periods=1).mean()/1e3\n",
    "min_time = df['time'].min()\n",
    "df['relative_time_s'] = (df['time'] - min_time).dt.total_seconds()\n",
    "\n",
    "df_zoomed = df[(df['time'] >= start_time) & (df['time'] <= end_time)]\n",
    "\n",
    "failure_ev : datetime = None\n",
    "viewchange_ev : datetime = None\n",
    "with open('../results/e2e_2000/client_node11_0.log', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if 'rpc error:' in line and failure_ev is None:\n",
    "            failure_ev = datetime.strptime(line.split(' ')[2], '%H:%M:%S.%f')\n",
    "            print(failure_ev)\n",
    "        elif 'mis-spec' in line and viewchange_ev is None:\n",
    "            viewchange_ev = datetime.strptime(line.split(' ')[2], '%H:%M:%S.%f')\n",
    "            print(viewchange_ev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(df_zoomed['relative_time_s'], df_zoomed['e2e_mov_avg'], label='e2e latency')\n",
    "# ax.plot(df_zoomed['relative_time_s'], df_zoomed['app_mov_avg'], color='grey', label='append latency')\n",
    "ax.axvline((failure_ev - min_time).total_seconds(), color='brown', alpha=0.7, linestyle='--', label=\"shard fail\")\n",
    "ax.axvline((viewchange_ev - min_time).total_seconds(), color='crimson', alpha=0.7, linestyle='--', label=\"view change notified\")\n",
    "ax.set_xlim(left=zoom[0], right=zoom[1])\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlabel('Time (seconds)')\n",
    "ax.set_ylabel('e2e latency (ms)')\n",
    "ax.set_title('e2e Latency change (moving avg, window=10)')\n",
    "ax.grid()\n",
    "\n",
    "plot.legend()\n",
    "plot.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tput = []\n",
    "time : 'list[datetime]' = []\n",
    "event : 'dict[str, tuple[datetime, str]]' = {}\n",
    "with open('../results/e2e_2000/order-0.log', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if '[real-time tput]' in line:\n",
    "            tput.append(int(line.split(' ')[-2]))\n",
    "            time.append(datetime.strptime(line.split(' ')[-5], '%H:%M:%S.%f'))\n",
    "        if '[last cut]'  in line:\n",
    "            event['last cut report'] = (datetime.strptime(line.split(' ')[-1].rstrip(), '%H:%M:%S.%f'), 'red')\n",
    "        elif '[lag detected]' in line:\n",
    "            event['significant lag detected'] = (datetime.strptime(line.split(' ')[-1].rstrip(), '%H:%M:%S.%f'), 'green')\n",
    "        elif '[fail detected]' in line:\n",
    "            print(line.split(' ')[-1])\n",
    "            event['shard failure detected'] = (datetime.strptime(line.split(' ')[-1].rstrip(), '%H:%M:%S.%f'), 'purple')\n",
    "        elif '[cut commit]' in line:\n",
    "            event['notify data servers'] = (datetime.strptime(line.split(' ')[-1].rstrip(), '%H:%M:%S.%f'), 'orange')\n",
    "\n",
    "start_time = min_time\n",
    "time_in_s = [(t - start_time).total_seconds() for t in time]\n",
    "\n",
    "plot, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(time_in_s, tput)\n",
    "ax.set_xlim(left=zoom[0], right=zoom[1])\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.grid(True)\n",
    "ax.set_xlabel(\"Time (seconds)\")\n",
    "ax.set_ylabel(\"Throughput (ops/sec)\")\n",
    "ax.set_title(\"Throughput change\")\n",
    "\n",
    "\n",
    "for e, t in sorted(event.items(), key=lambda it: it[1][0]):  # sort by time\n",
    "    print(e, t[0])\n",
    "    ax.axvline((t[0]-start_time).total_seconds(), color=t[1], alpha=0.7, linestyle='--', label=e)\n",
    "\n",
    "\n",
    "plot.legend()\n",
    "plot.tight_layout()\n",
    "plot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
