{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze e2e latencies for the single_client_e2e experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def get_append_metrics(path):\n",
    "    file_pattern = path + \"append_metrics*.csv\"\n",
    "\n",
    "    total_throughput = 0\n",
    "    latency_values = []\n",
    "\n",
    "    for file in glob.glob(file_pattern):\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()[1:]\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                gsn, latency, throughput = int(parts[0]), float(parts[1]), float(parts[2])\n",
    "                latency_values.append(latency)\n",
    "            \n",
    "            total_throughput += throughput\n",
    "\n",
    "    latency_array = np.array(latency_values)\n",
    "\n",
    "    mean_latency = np.mean(latency_array)\n",
    "    p50_latency = np.percentile(latency_array, 50)\n",
    "    p99_latency = np.percentile(latency_array, 99)\n",
    "\n",
    "    print(\"results for computation time \" + path.split(\"_\")[-1].split(\"/\")[0] + \" us\")\n",
    "    print(\"statistic/metric, latency (us)\")\n",
    "    print(f\"mean, {mean_latency:.2f}\")\n",
    "    print(f\"p50, {p50_latency:.2f}\")\n",
    "    print(f\"p99, {p99_latency:.2f}\")\n",
    "    print(f\"total throughput, {total_throughput:.2f}\")\n",
    "\n",
    "    return int(path.split(\"_\")[-1].split(\"/\")[0]), mean_latency, total_throughput \n",
    "\n",
    "def get_e2e_metrics(path):\n",
    "    file_pattern = path + \"e2e_metrics*.csv\"\n",
    "\n",
    "    delivery_latency_values = []\n",
    "    e2e_latency_values = []\n",
    "    queuing_delay_values = []\n",
    "\n",
    "    for file in glob.glob(file_pattern):\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()[1:]\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                delivery, e2e, queuing_delay = float(parts[1]), float(parts[2]), float(parts[3])\n",
    "                delivery_latency_values.append(delivery)\n",
    "                e2e_latency_values.append(e2e)\n",
    "                queuing_delay_values.append(queuing_delay)\n",
    "\n",
    "    \n",
    "    delivery_latency_array = np.array(delivery_latency_values)\n",
    "    e2e_latency_array = np.array(e2e_latency_values)\n",
    "    queuing_delay_array = np.array([x for x in queuing_delay_values if x > 0])\n",
    "\n",
    "    print(\"statistic/metric, delivery latency (us), e2e latency (us), queuing delay (us)\")\n",
    "    print(f\"mean, {np.mean(delivery_latency_array):.2f}, {np.mean(e2e_latency_array):.2f}, {np.mean(queuing_delay_array):.2f}\")\n",
    "    print(f\"std, {np.std(delivery_latency_array):.2f}, {np.std(e2e_latency_array):.2f}, {np.std(queuing_delay_array):.2f}\")\n",
    "    print(f\"p50, {np.percentile(delivery_latency_array, 50):.2f}, {np.percentile(e2e_latency_array, 50):.2f}, {np.percentile(queuing_delay_array, 50):.2f}\")\n",
    "    print(f\"p99, {np.percentile(delivery_latency_array, 99):.2f}, {np.percentile(e2e_latency_array, 99):.2f}, {np.percentile(queuing_delay_array, 99):.2f}\")\n",
    "\n",
    "    return int(path.split(\"_\")[-1].split(\"/\")[0]), np.mean(delivery_latency_array), np.mean(e2e_latency_array), np.mean(queuing_delay_array)\n",
    "\n",
    "\n",
    "def get_splits(path):\n",
    "    file_pattern = path + \"e2e_metrics*.csv\"\n",
    "\n",
    "    delivery_latency_values = []\n",
    "    e2e_latency_values = []\n",
    "    queuing_delay_values = []\n",
    "\n",
    "    for file in glob.glob(file_pattern):\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()[1:]\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                delivery, e2e, queuing_delay = float(parts[1]), float(parts[2]), float(parts[3])\n",
    "                delivery_latency_values.append(delivery)\n",
    "                e2e_latency_values.append(e2e)\n",
    "                queuing_delay_values.append(queuing_delay)\n",
    "\n",
    "    \n",
    "    delivery_latency_array = np.array(delivery_latency_values)\n",
    "    e2e_latency_array = np.array(e2e_latency_values)\n",
    "    queuing_delay_array = np.array([x for x in queuing_delay_values if x > 0])\n",
    "\n",
    "    min_size = min(len(delivery_latency_array), len(e2e_latency_array), len(queuing_delay_array))\n",
    "\n",
    "    delivery_latency_array = delivery_latency_array[:min_size]\n",
    "    e2e_latency_array = e2e_latency_array[:min_size]\n",
    "    queuing_delay_array = queuing_delay_array[:min_size]\n",
    "\n",
    "    compute = e2e_latency_array - delivery_latency_array - queuing_delay_array\n",
    "\n",
    "    print(\"results for computation time \" + path.split(\"_\")[-1].split(\"/\")[0] + \" us\")\n",
    "    print(\"statistic/metric, delivery latency (us), computation time (us), queuing delay (us)\")\n",
    "    print(f\"mean, {np.mean(delivery_latency_array):.2f}, {np.mean(compute):.2f}, {np.mean(queuing_delay_array):.2f}\")\n",
    "    print(f\"p50, {np.percentile(delivery_latency_array, 50):.2f}, {np.percentile(compute, 50):.2f}, {np.percentile(queuing_delay_array, 50):.2f}\")\n",
    "    print(f\"p99, {np.percentile(delivery_latency_array, 99):.2f}, {np.percentile(compute, 99):.2f}, {np.percentile(queuing_delay_array, 99):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../results/\"\n",
    "\n",
    "for dir in glob.glob(path + \"*/\"):\n",
    "    get_append_metrics(dir)\n",
    "    get_e2e_metrics(dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../results/e2e_4shard_scalog/*\"\n",
    "for dir in glob.glob(path + \"*/\"):\n",
    "    get_splits(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Input data\n",
    "data = \"\"\"\n",
    "PLEASE PASTE ABOVE DATA HERE\n",
    "\"\"\"\n",
    "\n",
    "# Split data into lines\n",
    "lines = data.splitlines()\n",
    "\n",
    "# Parsing logic\n",
    "results = []\n",
    "current_time = None\n",
    "append_latency = None\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    line = line.strip()\n",
    "    # Detect computation time\n",
    "    if line.startswith(\"results for computation time\"):\n",
    "        match = re.search(r\"computation time (\\d+) us\", line)\n",
    "        if match:\n",
    "            current_time = int(match.group(1))\n",
    "    # Extract append latency (first \"latency (us)\" mean value)\n",
    "    elif \"statistic/metric, latency (us)\" in line:\n",
    "        append_line = lines[i + 1]  # Look at the next line\n",
    "        append_match = re.match(r\"mean,\\s*([\\d.]+)\", append_line)\n",
    "        if append_match:\n",
    "            append_latency = float(append_match.group(1))\n",
    "    # Extract other latencies (mean values)\n",
    "    elif line.startswith(\"mean,\") and current_time is not None:\n",
    "        parts = line.split(\",\")\n",
    "        if len(parts) == 4:  # Ensure correct format\n",
    "            delivery_latency, e2e_latency, queueing_delay = map(float, parts[1:])\n",
    "            results.append({\n",
    "                \"computation_time\": current_time,\n",
    "                \"append_latency\": append_latency,\n",
    "                \"delivery_latency\": delivery_latency,\n",
    "                \"e2e_latency\": e2e_latency,\n",
    "                \"queueing_delay\": queueing_delay,\n",
    "            })\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.to_csv(\"output.csv\", index=False)\n",
    "\n",
    "# Print the CSV content\n",
    "print(df.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze order server log from the reconfiguration experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def analyze_reconfig_log(path):\n",
    "    # Sample log data (you can replace this with the contents of your log file)\n",
    "    with open(path, 'r') as f:\n",
    "        log_data = f.read()\n",
    "\n",
    "    tput = []\n",
    "    for line in log_data.splitlines():\n",
    "        if \"[real-time tput]:\" in line:\n",
    "            tput.append(int(line.split(\"ops/sec\")[0].split()[-1]))\n",
    "\n",
    "    \n",
    "    return tput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tputs = analyze_reconfig_log(\"../results/reconfig_1000/order-0.log\")\n",
    "# tputs = tputs[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# File path\n",
    "log_file = \"../results/reconfig_1000/order-0.log\"\n",
    "\n",
    "# Regex patterns\n",
    "tput_pattern = r\"\\[real-time tput\\]: (\\d+) ops/sec\"\n",
    "timestamp_pattern = r\"(\\d{2}:\\d{2}:\\d{2}\\.\\d{6})\"\n",
    "\n",
    "# Data storage\n",
    "timestamps = []\n",
    "tput_values = []\n",
    "shard_added = [] # when did the shards send their first cut to the OL\n",
    "first_cut_committed = [] # when did the first cut get committed\n",
    "shard_leave_request = [] \n",
    "shard_finalized = []\n",
    "replica_2_added = []\n",
    "replica_3_added = []\n",
    "replica_2_committed = []\n",
    "replica_3_committed = []\n",
    "\n",
    "# Parse the log file\n",
    "with open(log_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        # Extract real-time throughput\n",
    "        tput_match = re.search(tput_pattern, line)\n",
    "        timestamp_match = re.search(timestamp_pattern, line)\n",
    "        \n",
    "        if tput_match and timestamp_match:\n",
    "            tput_values.append(int(tput_match.group(1)))\n",
    "            timestamps.append(datetime.strptime(timestamp_match.group(1), \"%H:%M:%S.%f\"))\n",
    "        \n",
    "        # Extract events for annotation\n",
    "        if replica_2_added == [] and \"Replica 2 added\" in line:\n",
    "            replica_2_added.append(timestamp_match.group(1))\n",
    "\n",
    "        if replica_3_added == [] and \"Replica 3 added\" in line:\n",
    "            replica_3_added.append(timestamp_match.group(1))\n",
    "\n",
    "        if replica_2_committed == [] and \"cut:<key:2\" in line:\n",
    "            replica_2_committed.append(timestamp_match.group(1))\n",
    "        \n",
    "        if replica_3_committed == [] and \"cut:<key:3\" in line:\n",
    "            replica_3_committed.append(timestamp_match.group(1))\n",
    "        \n",
    "        if shard_finalized == [] and \"finalizeShards:<shardIDs:1 >\" in line:\n",
    "            shard_finalized.append(timestamp_match.group(1))\n",
    "\n",
    "        if shard_leave_request == [] and \"Shard 1 to be finalized\" in line:\n",
    "            shard_leave_request.append(timestamp_match.group(1))\n",
    "\n",
    "shard_added.append(max(replica_2_added[0], replica_3_added[0]))\n",
    "first_cut_committed.append(max(replica_2_committed[0], replica_3_committed[0]))\n",
    "\n",
    "# Convert timestamps to seconds since the start\n",
    "start_time = timestamps[0]\n",
    "time_in_seconds = [(ts - start_time).total_seconds() for ts in timestamps]\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_in_seconds, tput_values, label=\"Throughput (ops/sec)\", color=\"blue\")\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.ylabel(\"Throughput (ops/sec)\")\n",
    "plt.title(\"Real-time Throughput vs Time\")\n",
    "plt.grid()\n",
    "\n",
    "# Event times\n",
    "shard_join_request = (datetime.strptime(shard_added[0], \"%H:%M:%S.%f\") - start_time).total_seconds()\n",
    "first_cut_committed_time = (datetime.strptime(first_cut_committed[0], \"%H:%M:%S.%f\") - start_time).total_seconds()\n",
    "shard_leave_request_time = (datetime.strptime(shard_leave_request[0], \"%H:%M:%S.%f\") - start_time).total_seconds()\n",
    "shard_finalized_time = (datetime.strptime(shard_finalized[0], \"%H:%M:%S.%f\") - start_time).total_seconds()\n",
    "\n",
    "# Add vertical lines for events\n",
    "plt.axvline(shard_join_request, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"shard requests to join\")\n",
    "plt.axvline(first_cut_committed_time, color=\"purple\", linestyle=\"--\", alpha=0.7, label=\"first cut committed from new shard\")\n",
    "plt.axvline(shard_leave_request_time, color=\"orange\", linestyle=\"--\", alpha=0.7, label=\"shard requests to leave\")\n",
    "plt.axvline(shard_finalized_time, color=\"black\", linestyle=\"--\", alpha=0.7, label=\"shard finalized, last committed cut\")\n",
    "\n",
    "# Zoom in to the relevant range (adjust as needed)\n",
    "# plt.xlim(22.5, 24)\n",
    "# plt.xlim(10, 65)\n",
    "plt.xlim(52, 55)\n",
    "\n",
    "# Add legend outside the plot area\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), title=\"Events\")\n",
    "\n",
    "# Adjust layout to ensure the legend doesn't overlap with the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"adding_reconfig_throughput_annotated.png\", dpi=600)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze real-time tput from emulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Helper function to parse timestamps\n",
    "def parse_timestamp(ts):\n",
    "    return datetime.strptime(ts, \"%H:%M:%S.%f\")\n",
    "\n",
    "# File path\n",
    "log_file = \"../results/emulation_20/order-0.log\"\n",
    "\n",
    "# Regex patterns\n",
    "tput_pattern = r\"\\[real-time tput\\]: (\\d+) ops/sec\"\n",
    "timestamp_pattern = r\"(\\d{2}:\\d{2}:\\d{2}\\.\\d{6})\"\n",
    "\n",
    "timestamps = []\n",
    "tput_values = []\n",
    "\n",
    "with open(log_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        # Extract throughput\n",
    "        tput_match = re.search(tput_pattern, line)\n",
    "        timestamp_match = re.search(timestamp_pattern, line)\n",
    "        \n",
    "        if tput_match and timestamp_match:\n",
    "            tput_values.append(int(tput_match.group(1)))\n",
    "            timestamps.append(parse_timestamp(timestamp_match.group(1)))\n",
    "\n",
    "\n",
    "min_timestamp = min(timestamps)\n",
    "\n",
    "def to_relative_ms(t):\n",
    "    return (t - min_timestamp).total_seconds() * 1000\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot throughput\n",
    "ax.plot([to_relative_ms(t) for t in timestamps], tput_values, label=\"Throughput (ops/sec)\", color=\"blue\")\n",
    "\n",
    "# Labels and legend\n",
    "ax.set_xlabel(\"Relative Time (ms)\")\n",
    "ax.set_ylabel(\"Throughput (ops/sec)\")\n",
    "ax.set_title(\"Throughput vs Time\")\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"emulation_tput.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Helper function to compute moving average\n",
    "def moving_average(values, window_size):\n",
    "    return np.convolve(values, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "def parse_timestamp(ts):\n",
    "    return datetime.strptime(ts, \"%H:%M:%S.%f\")\n",
    "\n",
    "def get_mean_tput_filtered(path):\n",
    "    # File path\n",
    "    log_file = path + \"order-0.log\"\n",
    "\n",
    "    # Regex patterns\n",
    "    tput_pattern = r\"\\[real-time tput\\]: (\\d+) ops/sec\"\n",
    "    timestamp_pattern = r\"(\\d{2}:\\d{2}:\\d{2}\\.\\d{6})\"\n",
    "\n",
    "    timestamps = []\n",
    "    tput_values = []\n",
    "\n",
    "    with open(log_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Extract throughput\n",
    "            tput_match = re.search(tput_pattern, line)\n",
    "            total_tput_match = re.search(total_tput_pattern, line)\n",
    "            timestamp_match = re.search(timestamp_pattern, line)\n",
    "            \n",
    "            if tput_match and timestamp_match:\n",
    "                tput_values.append(int(tput_match.group(1)))\n",
    "                timestamps.append(parse_timestamp(timestamp_match.group(1)))\n",
    "                \n",
    "    min_timestamp = min(timestamps)\n",
    "\n",
    "    def to_relative_ms(t):\n",
    "        return (t - min_timestamp).total_seconds() * 1000\n",
    "\n",
    "    \n",
    "    # Choose smoothing method: Moving Average or Gaussian\n",
    "    window_size = 20  # Adjust window size for smoothing\n",
    "    smoothed_tput = moving_average(tput_values, window_size)\n",
    "\n",
    "    # Adjust timestamps for the reduced size after smoothing\n",
    "    smoothed_timestamps = [to_relative_ms(t) for t in timestamps][window_size-1:]\n",
    "\n",
    "    # # Plot smoothed data\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(smoothed_timestamps, smoothed_tput, label=\"Smoothed Throughput (ops/sec)\", color=\"blue\")\n",
    "    # plt.plot(smoothed_total_timestamps, smoothed_total_tput, label=\"Smoothed Total Throughput (ops/sec)\", color=\"green\")\n",
    "\n",
    "    # plt.xlabel(\"Relative Time (ms)\")\n",
    "    # plt.ylabel(\"Throughput (ops/sec)\")\n",
    "    # plt.title(\"Smoothed Real-time Throughput vs Time\")\n",
    "    # plt.grid()\n",
    "    # plt.legend()\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(\"smoothed_output.png\", dpi=600)\n",
    "\n",
    "    # Filter throughput data based on relative time condition\n",
    "    filtered_timestamps = []\n",
    "    filtered_tput_values = []\n",
    "\n",
    "    for t, v in zip(smoothed_timestamps, smoothed_tput):\n",
    "        relative_time = t\n",
    "        if 25000 <= relative_time <= 130000:\n",
    "            filtered_timestamps.append(t)\n",
    "            filtered_tput_values.append(v)\n",
    "\n",
    "    return np.mean(filtered_tput_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15,292098.3361904762\n",
      "20,389823.5238095238\n",
      "40,738037.1152380953\n",
      "5,97653.64761904762\n",
      "30,582040.8633333333\n",
      "35,673107.0180952381\n",
      "10,194921.69476190477\n",
      "25,487563.54285714286\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "for file in glob.glob(\"../results/emulation_*/\"):\n",
    "    mean_tput = get_mean_tput_filtered(file)\n",
    "    print(file.split(\"/\")[-2].split(\"_\")[1] + \",\" + str(mean_tput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fnmatch\n",
    "\n",
    "def average_emulation_metrics(directory):\n",
    "    \"\"\"\n",
    "    Calculates the average of the `mean`, `p50`, and `p99` metrics across all files in a directory matching the pattern `data*.log`.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing log files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the averages of `mean`, `p50`, and `p99` metrics.\n",
    "    \"\"\"\n",
    "    data_regex = re.compile(r\"(mean|p50|p99),\\s*([\\d.e+-]+),\\s*([\\d.e+-]+)\")\n",
    "    \n",
    "    append_metrics = {\"mean\": [], \"p50\": [], \"p99\": []}\n",
    "    delivery_metrics = {\"mean\": [], \"p50\": [], \"p99\": []}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if fnmatch.fnmatch(filename, 'data*.log'):  \n",
    "            filepath = os.path.join(directory, filename)\n",
    "            \n",
    "            # Temporary dictionaries to track the last occurrence in the current file\n",
    "            last_append_metrics = {}\n",
    "            last_delivery_metrics = {}\n",
    "\n",
    "            with open(filepath, 'r') as file:\n",
    "                for line in file:\n",
    "                    match = data_regex.search(line)\n",
    "                    if match:\n",
    "                        metric, append_latency, delivery_latency = match.groups()\n",
    "                        # Update the last seen values for the current file\n",
    "                        last_append_metrics[metric] = float(append_latency)\n",
    "                        last_delivery_metrics[metric] = float(delivery_latency)\n",
    "\n",
    "            # Append the last occurrence values from the current file to the global lists\n",
    "            for metric in append_metrics.keys():\n",
    "                if metric in last_append_metrics:\n",
    "                    append_metrics[metric].append(last_append_metrics[metric])\n",
    "                if metric in last_delivery_metrics:\n",
    "                    delivery_metrics[metric].append(last_delivery_metrics[metric])\n",
    "\n",
    "    # Compute averages across all files for the last occurrences\n",
    "    averages = {\n",
    "        \"append\": {\n",
    "            metric: sum(values) / len(values) if values else None\n",
    "            for metric, values in append_metrics.items()\n",
    "        },\n",
    "        \"delivery\": {\n",
    "            metric: sum(values) / len(values) if values else None\n",
    "            for metric, values in delivery_metrics.items()\n",
    "        }\n",
    "    }\n",
    "    print(\"found values across \" + str(len(append_metrics[\"mean\"])) + \" files\")\n",
    "    \n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average metrics for 5 shards:\n",
      "found values across 10 files\n",
      "append/confirmation latency (us):\n",
      "mean: 2575.677237815214\n",
      "p50: 2573.6\n",
      "p99: 3469.3\n",
      "delivery latency (us):\n",
      "mean: 2576.714402993392\n",
      "p50: 2574.5\n",
      "p99: 3470.7\n",
      "\n",
      "Average metrics for 10 shards:\n",
      "found values across 20 files\n",
      "append/confirmation latency (us):\n",
      "mean: 2559.802638455046\n",
      "p50: 2557.6\n",
      "p99: 3634.95\n",
      "delivery latency (us):\n",
      "mean: 2560.8813955902274\n",
      "p50: 2558.65\n",
      "p99: 3636.4\n",
      "\n",
      "Average metrics for 15 shards:\n",
      "found values across 30 files\n",
      "append/confirmation latency (us):\n",
      "mean: 2688.454130050981\n",
      "p50: 2649.8333333333335\n",
      "p99: 4031.133333333333\n",
      "delivery latency (us):\n",
      "mean: 2689.5205374509756\n",
      "p50: 2650.766666666667\n",
      "p99: 4032.733333333333\n",
      "\n",
      "Average metrics for 20 shards:\n",
      "found values across 40 files\n",
      "append/confirmation latency (us):\n",
      "mean: 2669.032442097973\n",
      "p50: 2649.5\n",
      "p99: 3973.075\n",
      "delivery latency (us):\n",
      "mean: 2670.019698822411\n",
      "p50: 2650.425\n",
      "p99: 3974.425\n",
      "\n",
      "Average metrics for 25 shards:\n",
      "found values across 50 files\n",
      "append/confirmation latency (us):\n",
      "mean: 2679.949827879223\n",
      "p50: 2633.26\n",
      "p99: 4248.4\n",
      "delivery latency (us):\n",
      "mean: 2680.8351682223933\n",
      "p50: 2634.08\n",
      "p99: 4249.6\n",
      "\n",
      "Average metrics for 30 shards:\n",
      "found values across 60 files\n",
      "append/confirmation latency (us):\n",
      "mean: 2827.2862227630944\n",
      "p50: 2725.366666666667\n",
      "p99: 5296.4\n",
      "delivery latency (us):\n",
      "mean: 2828.0622288379122\n",
      "p50: 2726.1\n",
      "p99: 5297.866666666667\n",
      "\n",
      "Average metrics for 35 shards:\n",
      "found values across 70 files\n",
      "append/confirmation latency (us):\n",
      "mean: 2852.5962943117993\n",
      "p50: 2704.1\n",
      "p99: 6088.842857142857\n",
      "delivery latency (us):\n",
      "mean: 2853.327002167525\n",
      "p50: 2704.714285714286\n",
      "p99: 6090.2\n",
      "\n",
      "Average metrics for 40 shards:\n",
      "found values across 80 files\n",
      "append/confirmation latency (us):\n",
      "mean: 3358.6379043636757\n",
      "p50: 3015.1125\n",
      "p99: 7960.9875\n",
      "delivery latency (us):\n",
      "mean: 3359.359690787879\n",
      "p50: 3015.775\n",
      "p99: 7962.225\n"
     ]
    }
   ],
   "source": [
    "for num_shards in [5, 10, 15, 20, 25, 30, 35, 40]:\n",
    "    # Define input directory\n",
    "    input_directory = \"../results/emulation_\" + str(num_shards) + \"/\"\n",
    "\n",
    "    print(f\"\\nAverage metrics for {num_shards} shards:\")\n",
    "    # Calculate averages\n",
    "    averages = average_emulation_metrics(input_directory)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"append/confirmation latency (us):\")\n",
    "    print(f\"mean: {averages['append']['mean']}\")\n",
    "    print(f\"p50: {averages['append']['p50']}\")\n",
    "    print(f\"p99: {averages['append']['p99']}\")\n",
    "\n",
    "    print(\"delivery latency (us):\")\n",
    "    print(f\"mean: {averages['delivery']['mean']}\")\n",
    "    print(f\"p50: {averages['delivery']['p50']}\")\n",
    "    print(f\"p99: {averages['delivery']['p99']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
