{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze e2e latencies for the single_client_e2e experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def get_append_metrics(path):\n",
    "    file_pattern = path + \"append_metrics*.csv\"\n",
    "\n",
    "    total_throughput = 0\n",
    "    latency_values = []\n",
    "\n",
    "    for file in glob.glob(file_pattern):\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()[1:]\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                gsn, latency, throughput = int(parts[0]), float(parts[1]), float(parts[2])\n",
    "                latency_values.append(latency)\n",
    "            \n",
    "            total_throughput += throughput\n",
    "\n",
    "    latency_array = np.array(latency_values)\n",
    "\n",
    "    mean_latency = np.mean(latency_array)\n",
    "    p50_latency = np.percentile(latency_array, 50)\n",
    "    p99_latency = np.percentile(latency_array, 99)\n",
    "\n",
    "    print(\"results for computation time \" + path.split(\"_\")[-1].split(\"/\")[0] + \" us\")\n",
    "    print(\"statistic/metric, latency (us)\")\n",
    "    print(f\"mean, {mean_latency:.2f}\")\n",
    "    print(f\"p50, {p50_latency:.2f}\")\n",
    "    print(f\"p99, {p99_latency:.2f}\")\n",
    "    print(f\"total throughput, {total_throughput:.2f}\")\n",
    "\n",
    "    return int(path.split(\"_\")[-1].split(\"/\")[0]), mean_latency, total_throughput \n",
    "\n",
    "def get_e2e_metrics(path):\n",
    "    file_pattern = path + \"e2e_metrics*.csv\"\n",
    "\n",
    "    delivery_latency_values = []\n",
    "    e2e_latency_values = []\n",
    "    queuing_delay_values = []\n",
    "\n",
    "    for file in glob.glob(file_pattern):\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()[1:]\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                delivery, e2e, queuing_delay = float(parts[1]), float(parts[2]), float(parts[3])\n",
    "                delivery_latency_values.append(delivery)\n",
    "                e2e_latency_values.append(e2e)\n",
    "                queuing_delay_values.append(queuing_delay)\n",
    "\n",
    "    \n",
    "    delivery_latency_array = np.array(delivery_latency_values)\n",
    "    e2e_latency_array = np.array(e2e_latency_values)\n",
    "    queuing_delay_array = np.array([x for x in queuing_delay_values if x > 0])\n",
    "\n",
    "    print(\"statistic/metric, delivery latency (us), e2e latency (us), queuing delay (us)\")\n",
    "    print(f\"mean, {np.mean(delivery_latency_array):.2f}, {np.mean(e2e_latency_array):.2f}, {np.mean(queuing_delay_array):.2f}\")\n",
    "    print(f\"std, {np.std(delivery_latency_array):.2f}, {np.std(e2e_latency_array):.2f}, {np.std(queuing_delay_array):.2f}\")\n",
    "    print(f\"p50, {np.percentile(delivery_latency_array, 50):.2f}, {np.percentile(e2e_latency_array, 50):.2f}, {np.percentile(queuing_delay_array, 50):.2f}\")\n",
    "    print(f\"p99, {np.percentile(delivery_latency_array, 99):.2f}, {np.percentile(e2e_latency_array, 99):.2f}, {np.percentile(queuing_delay_array, 99):.2f}\")\n",
    "\n",
    "    return int(path.split(\"_\")[-1].split(\"/\")[0]), np.mean(delivery_latency_array), np.mean(e2e_latency_array), np.mean(queuing_delay_array)\n",
    "\n",
    "\n",
    "def get_splits(path):\n",
    "    file_pattern = path + \"e2e_metrics*.csv\"\n",
    "\n",
    "    delivery_latency_values = []\n",
    "    e2e_latency_values = []\n",
    "    queuing_delay_values = []\n",
    "\n",
    "    for file in glob.glob(file_pattern):\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()[1:]\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                delivery, e2e, queuing_delay = float(parts[1]), float(parts[2]), float(parts[3])\n",
    "                delivery_latency_values.append(delivery)\n",
    "                e2e_latency_values.append(e2e)\n",
    "                queuing_delay_values.append(queuing_delay)\n",
    "\n",
    "    \n",
    "    delivery_latency_array = np.array(delivery_latency_values)\n",
    "    e2e_latency_array = np.array(e2e_latency_values)\n",
    "    queuing_delay_array = np.array([x for x in queuing_delay_values if x > 0])\n",
    "\n",
    "    min_size = min(len(delivery_latency_array), len(e2e_latency_array), len(queuing_delay_array))\n",
    "\n",
    "    delivery_latency_array = delivery_latency_array[:min_size]\n",
    "    e2e_latency_array = e2e_latency_array[:min_size]\n",
    "    queuing_delay_array = queuing_delay_array[:min_size]\n",
    "\n",
    "    compute = e2e_latency_array - delivery_latency_array - queuing_delay_array\n",
    "\n",
    "    print(\"results for computation time \" + path.split(\"_\")[-1].split(\"/\")[0] + \" us\")\n",
    "    print(\"statistic/metric, delivery latency (us), computation time (us), queuing delay (us)\")\n",
    "    print(f\"mean, {np.mean(delivery_latency_array):.2f}, {np.mean(compute):.2f}, {np.mean(queuing_delay_array):.2f}\")\n",
    "    print(f\"p50, {np.percentile(delivery_latency_array, 50):.2f}, {np.percentile(compute, 50):.2f}, {np.percentile(queuing_delay_array, 50):.2f}\")\n",
    "    print(f\"p99, {np.percentile(delivery_latency_array, 99):.2f}, {np.percentile(compute, 99):.2f}, {np.percentile(queuing_delay_array, 99):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../results/*\"\n",
    "\n",
    "for dir in glob.glob(path + \"*/\"):\n",
    "    get_append_metrics(dir)\n",
    "    get_e2e_metrics(dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../results/*\"\n",
    "for dir in glob.glob(path + \"*/\"):\n",
    "    get_splits(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Input data\n",
    "data = \"\"\"\n",
    "PLEASE PASTE ABOVE DATA HERE\n",
    "\"\"\"\n",
    "\n",
    "# Split data into lines\n",
    "lines = data.splitlines()\n",
    "\n",
    "# Parsing logic\n",
    "results = []\n",
    "current_time = None\n",
    "append_latency = None\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    line = line.strip()\n",
    "    # Detect computation time\n",
    "    if line.startswith(\"results for computation time\"):\n",
    "        match = re.search(r\"computation time (\\d+) us\", line)\n",
    "        if match:\n",
    "            current_time = int(match.group(1))\n",
    "    # Extract append latency (first \"latency (us)\" mean value)\n",
    "    elif \"statistic/metric, latency (us)\" in line:\n",
    "        append_line = lines[i + 1]  # Look at the next line\n",
    "        append_match = re.match(r\"mean,\\s*([\\d.]+)\", append_line)\n",
    "        if append_match:\n",
    "            append_latency = float(append_match.group(1))\n",
    "    # Extract other latencies (mean values)\n",
    "    elif line.startswith(\"mean,\") and current_time is not None:\n",
    "        parts = line.split(\",\")\n",
    "        if len(parts) == 4:  # Ensure correct format\n",
    "            delivery_latency, e2e_latency, queueing_delay = map(float, parts[1:])\n",
    "            results.append({\n",
    "                \"computation_time\": current_time,\n",
    "                \"append_latency\": append_latency,\n",
    "                \"delivery_latency\": delivery_latency,\n",
    "                \"e2e_latency\": e2e_latency,\n",
    "                \"queueing_delay\": queueing_delay,\n",
    "            })\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.to_csv(\"output.csv\", index=False)\n",
    "\n",
    "# Print the CSV content\n",
    "print(df.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze order server log from the reconfiguration experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# File path\n",
    "log_file = \"../results/reconfig_800/order-0.log\"\n",
    "\n",
    "# Regex patterns\n",
    "tput_pattern = r\"\\[real-time tput\\]: (\\d+) ops/sec\"\n",
    "timestamp_pattern = r\"(\\d{2}:\\d{2}:\\d{2}\\.\\d{6})\"\n",
    "\n",
    "# Data storage\n",
    "timestamps = []\n",
    "tput_values = []\n",
    "shard_added = [] # when did the shards send their first cut to the OL\n",
    "first_cut_committed = [] # when did the first cut get committed\n",
    "shard_leave_request = [] \n",
    "shard_finalized = []\n",
    "replica_2_added = []\n",
    "replica_3_added = []\n",
    "replica_2_committed = []\n",
    "replica_3_committed = []\n",
    "\n",
    "# Parse the log file\n",
    "with open(log_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        # Extract real-time throughput\n",
    "        tput_match = re.search(tput_pattern, line)\n",
    "        timestamp_match = re.search(timestamp_pattern, line)\n",
    "        \n",
    "        if tput_match and timestamp_match:\n",
    "            tput_values.append(int(tput_match.group(1)))\n",
    "            timestamps.append(datetime.strptime(timestamp_match.group(1), \"%H:%M:%S.%f\"))\n",
    "        \n",
    "        # Extract events for annotation\n",
    "        if replica_2_added == [] and \"Replica 2 added\" in line:\n",
    "            replica_2_added.append(timestamp_match.group(1))\n",
    "\n",
    "        if replica_3_added == [] and \"Replica 3 added\" in line:\n",
    "            replica_3_added.append(timestamp_match.group(1))\n",
    "\n",
    "        if replica_2_committed == [] and \"cut:<key:2\" in line:\n",
    "            replica_2_committed.append(timestamp_match.group(1))\n",
    "        \n",
    "        if replica_3_committed == [] and \"cut:<key:3\" in line:\n",
    "            replica_3_committed.append(timestamp_match.group(1))\n",
    "        \n",
    "        if shard_finalized == [] and \"finalizeShards:<shardIDs:1 >\" in line:\n",
    "            shard_finalized.append(timestamp_match.group(1))\n",
    "\n",
    "        if shard_leave_request == [] and \"Shard 1 to be finalized\" in line:\n",
    "            shard_leave_request.append(timestamp_match.group(1))\n",
    "\n",
    "shard_added.append(max(replica_2_added[0], replica_3_added[0]))\n",
    "first_cut_committed.append(max(replica_2_committed[0], replica_3_committed[0]))\n",
    "\n",
    "# Convert timestamps to seconds since the start\n",
    "start_time = timestamps[0]\n",
    "time_in_seconds = [(ts - start_time).total_seconds() for ts in timestamps]\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_in_seconds, tput_values, label=\"Throughput (ops/sec)\", color=\"blue\")\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.ylabel(\"Throughput (ops/sec)\")\n",
    "plt.title(\"Real-time Throughput vs Time\")\n",
    "plt.grid()\n",
    "\n",
    "# Event times\n",
    "shard_join_request = (datetime.strptime(shard_added[0], \"%H:%M:%S.%f\") - start_time).total_seconds()\n",
    "first_cut_committed_time = (datetime.strptime(first_cut_committed[0], \"%H:%M:%S.%f\") - start_time).total_seconds()\n",
    "shard_leave_request_time = (datetime.strptime(shard_leave_request[0], \"%H:%M:%S.%f\") - start_time).total_seconds()\n",
    "shard_finalized_time = (datetime.strptime(shard_finalized[0], \"%H:%M:%S.%f\") - start_time).total_seconds()\n",
    "\n",
    "# Add vertical lines for events\n",
    "plt.axvline(shard_join_request, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"shard requests to join\")\n",
    "plt.axvline(first_cut_committed_time, color=\"purple\", linestyle=\"--\", alpha=0.7, label=\"first cut committed from new shard\")\n",
    "plt.axvline(shard_leave_request_time, color=\"orange\", linestyle=\"--\", alpha=0.7, label=\"shard requests to leave\")\n",
    "plt.axvline(shard_finalized_time, color=\"black\", linestyle=\"--\", alpha=0.7, label=\"shard finalized, last committed cut\")\n",
    "\n",
    "# Zoom in to the relevant range (adjust as needed)\n",
    "# plt.xlim(22.5, 24)\n",
    "# plt.xlim(10, 65)\n",
    "plt.xlim(52, 55)\n",
    "\n",
    "# Add legend outside the plot area\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), title=\"Events\")\n",
    "\n",
    "# Adjust layout to ensure the legend doesn't overlap with the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"adding_reconfig_throughput_annotated.png\", dpi=600)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyze latencies for lagfix experiment\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import bisect\n",
    "\n",
    "timestamp_pattern = r\"(\\d{2}:\\d{2}:\\d{2}\\.\\d{6})\"\n",
    "\n",
    "\n",
    "def parse_timestamp(time):\n",
    "    return datetime.strptime(time, \"%H:%M:%S.%f\")\n",
    "\n",
    "def analyze_reconfig(path):\n",
    "    client_path = path + \"client_node13.log\"\n",
    "    e2e_path = path + \"e2e_metrics.csv\"\n",
    "\n",
    "    join_time = None\n",
    "    leave_time = None\n",
    "    # Determine join and leave GSN\n",
    "    with open(client_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if \"View id: 2\" in line and join_time is None:\n",
    "                join_time = re.search(timestamp_pattern, line).group(1)\n",
    "            if \"View id: 3\" in line and leave_time is None:\n",
    "                leave_time = re.search(timestamp_pattern, line).group(1)\n",
    "\n",
    "    # Parse e2e metrics\n",
    "    timestamps = []\n",
    "    latencies = []\n",
    "    gsns = []\n",
    "    with open(e2e_path, 'r') as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        for line in lines:\n",
    "            parts = line.strip().split(',')\n",
    "            gsn = int(parts[0])\n",
    "            e2e_latency = int(parts[2])\n",
    "            timestamp = parts[4]\n",
    "            gsns.append(gsn)\n",
    "            latencies.append(e2e_latency)\n",
    "            timestamps.append(datetime.strptime(timestamp, \"%H:%M:%S.%f\"))\n",
    "\n",
    "    # Prepare data for the plot\n",
    "    df = pd.DataFrame({\n",
    "        'time': timestamps,\n",
    "        'latency': latencies\n",
    "    })\n",
    "    df = df.sort_values(by='time')\n",
    "\n",
    "    # Calculate moving average\n",
    "    window_size = 500  # Set window size for moving average\n",
    "    df['moving_avg'] = df['latency'].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "    # Calculate relative time\n",
    "    min_time = df['time'].min()\n",
    "    df['relative_time_ms'] = (df['time'] - min_time).dt.total_seconds() * 1000\n",
    "\n",
    "    # Plot moving average and markers\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['relative_time_ms'], df['moving_avg'], label=f'Moving Average (window={window_size})', color='blue', linewidth=2)\n",
    "\n",
    "    # Plot join and leave markers\n",
    "    join_time_relative = (parse_timestamp(join_time) - min_time).total_seconds() * 1000\n",
    "    leave_time_relative = (parse_timestamp(leave_time) - min_time).total_seconds() * 1000\n",
    "    plt.axvline(x=join_time_relative, color='green', linestyle=':', label='Shard Added')\n",
    "    plt.axvline(x=leave_time_relative, color='red', linestyle=':', label='Shard Removed')\n",
    "\n",
    "    # Calculate the average latency\n",
    "    average_latency = df['latency'].mean()\n",
    "\n",
    "    # Plot the average latency as a horizontal line\n",
    "    plt.axhline(y=average_latency, color='orange', linestyle='--', label=f'Average Latency ({average_latency:.2f} µs)')\n",
    "\n",
    "\n",
    "    # Customize plot\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Latency (us)')\n",
    "    plt.title('Latency Over Time with Moving Average')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(ymin=0, ymax=9000)\n",
    "    # plt.xlim(xmin=15000, xmax=17000)\n",
    "    # plt.xlim(xmin=45000, xmax=46000)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    plt.savefig(\"output.png\", dpi=600)\n",
    "    print(\"Plot saved as output.png\")\n",
    "\n",
    "path = \"../results/reconfig_800_scalog/\"\n",
    "analyze_reconfig(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze script for lagfix plots for e2e and append latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# analyze logs to get latency metrics for lagfix experiment\n",
    "def get_latency_metrics_for_lagfix(path):\n",
    "    e2e_file_pattern = path + \"e2e_metrics.csv\"\n",
    "    append_file_pattern = path + \"append_latency_timestamp.csv\"\n",
    "    append_latency_values = {}\n",
    "    e2e_latency_values = {}\n",
    "    gsns = []\n",
    "\n",
    "    for file in glob.glob(e2e_file_pattern):\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()[1:]  # Skip the header\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                gsn, e2e_latency, delivery_timestamp = int(parts[0]), int(parts[2]), parts[4]\n",
    "                e2e_latency_values[gsn] = [e2e_latency, delivery_timestamp]\n",
    "\n",
    "\n",
    "    for file in glob.glob(append_file_pattern):\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()[1:]  # Skip the header\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                gsn, timestamp, latency = int(parts[0]), parts[1], int(parts[2])\n",
    "                append_latency_values[gsn] = [latency, timestamp]\n",
    "    \n",
    "    client_file = path + \"client_node7.log\"\n",
    "    with open(client_file, 'r') as f:\n",
    "        log_data = f.read()\n",
    "        for line in log_data.splitlines():\n",
    "            if \"gsn: \" in line:\n",
    "                try:\n",
    "                    gsn = int(line.split()[-1])  \n",
    "                    gsns.append(gsn)\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping invalid GSN: {line.split()[-1]}\")\n",
    "\n",
    "    gsns.sort()\n",
    "    return append_latency_values, e2e_latency_values, gsns\n",
    "\n",
    "\n",
    "path = \"../results/lagfix/\"\n",
    "append_latency_values, e2e_latency_values, gsns = get_latency_metrics_for_lagfix(path)\n",
    "\n",
    "append_latency_array = np.array([append_latency_values[gsn][0] for gsn in append_latency_values])\n",
    "mean_latency = np.mean(append_latency_array)\n",
    "p99_latency = np.percentile(append_latency_array, 99)\n",
    "\n",
    "e2e_latency_array = np.array([e2e_latency_values[gsn][0] for gsn in e2e_latency_values])\n",
    "mean_e2e_latency = np.mean(e2e_latency_array)\n",
    "p99_e2e_latency = np.percentile(e2e_latency_array, 99)\n",
    "\n",
    "print(f\"mean append latency: {mean_latency:.2f} us\")\n",
    "print(f\"p99 latency: {p99_latency:.2f} us\")\n",
    "print(f\"mean e2e latency: {mean_e2e_latency:.2f} us\")\n",
    "print(f\"p99 e2e latency: {p99_e2e_latency:.2f} us\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot append latency over time with moving average\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "append_latency_times = []\n",
    "append_latencies = []\n",
    "for gsn, (latency, timestamp) in append_latency_values.items():\n",
    "    append_latency_times.append(datetime.strptime(timestamp, \"%H:%M:%S.%f\"))\n",
    "    append_latencies.append(latency)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'time': append_latency_times,\n",
    "    'latency': append_latencies\n",
    "})\n",
    "\n",
    "df = df.sort_values(by='time')\n",
    "\n",
    "# CONFIGURE WINDOW SIZE HERE\n",
    "window_size = 10  # Set the window size for the moving average\n",
    "df['moving_avg'] = df['latency'].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "min_time = df['time'].min()\n",
    "df['relative_time_ms'] = (df['time'] - min_time).dt.total_seconds() * 1000\n",
    "\n",
    "# CONFIGURE ZOOM IN PERIOD HERE\n",
    "start_time = datetime.strptime(append_latency_values[gsns[0]][1], \"%H:%M:%S.%f\") - timedelta(milliseconds=100)  \n",
    "end_time = datetime.strptime(append_latency_values[gsns[-1]][1], \"%H:%M:%S.%f\") + timedelta(milliseconds=100)  \n",
    "\n",
    "df_zoomed = df[(df['time'] >= start_time) & (df['time'] <= end_time)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_zoomed['relative_time_ms'], df_zoomed['moving_avg'], label=f'Moving Average (window={window_size})', color='blue', linewidth=2)\n",
    "\n",
    "start_time_relative = (datetime.strptime(append_latency_values[gsns[0]][1], \"%H:%M:%S.%f\") - min_time).total_seconds() * 1000\n",
    "end_time_relative = (datetime.strptime(append_latency_values[gsns[-1]][1], \"%H:%M:%S.%f\") - min_time).total_seconds() * 1000\n",
    "plt.axvline(x=start_time_relative, color='green', linestyle=':', label=f'burst start time')\n",
    "plt.axvline(x=end_time_relative, color='red', linestyle=':', label=f'burst end time')\n",
    "\n",
    "plt.xlabel('time (ms)')\n",
    "plt.ylabel('latency (us)')\n",
    "plt.title('append latency over time with moving average')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(ymin=0, ymax=16000)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"append_without.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot e2e latency over time with moving average\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "e2e_latency_times = []\n",
    "e2e_latencies = []\n",
    "for gsn, (latency, timestamp) in e2e_latency_values.items():\n",
    "    e2e_latency_times.append(datetime.strptime(timestamp, \"%H:%M:%S.%f\"))\n",
    "    e2e_latencies.append(latency)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'time': e2e_latency_times,\n",
    "    'latency': e2e_latencies\n",
    "})\n",
    "\n",
    "df = df.sort_values(by='time')\n",
    "\n",
    "# CONFIGURE WINDOW SIZE HERE\n",
    "window_size = 10  # Set the window size for the moving average\n",
    "df['moving_avg'] = df['latency'].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "min_time = df['time'].min()\n",
    "df['relative_time_ms'] = (df['time'] - min_time).dt.total_seconds() * 1000\n",
    "\n",
    "# CONFIGURE ZOOM IN PERIOD HERE\n",
    "start_time = datetime.strptime(e2e_latency_values[gsns[0]][1], \"%H:%M:%S.%f\") - timedelta(milliseconds=100)  \n",
    "end_time = datetime.strptime(e2e_latency_values[gsns[-1]][1], \"%H:%M:%S.%f\") + timedelta(milliseconds=100)  \n",
    "\n",
    "df_zoomed = df[(df['time'] >= start_time) & (df['time'] <= end_time)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_zoomed['relative_time_ms'], df_zoomed['moving_avg'], label=f'Moving Average (window={window_size})', color='blue', linewidth=2)\n",
    "\n",
    "\n",
    "start_time_relative = (datetime.strptime(e2e_latency_values[gsns[0]][1], \"%H:%M:%S.%f\") - min_time).total_seconds() * 1000\n",
    "end_time_relative = (datetime.strptime(e2e_latency_values[gsns[-1]][1], \"%H:%M:%S.%f\") - min_time).total_seconds() * 1000\n",
    "plt.axvline(x=start_time_relative, color='green', linestyle=':', label=f'burst start time')\n",
    "plt.axvline(x=end_time_relative, color='red', linestyle=':', label=f'burst end time')\n",
    "\n",
    "\n",
    "plt.xlabel('time (ms)')\n",
    "plt.ylabel('latency (us)')\n",
    "plt.title('e2e latency over time with moving average')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(ymin=0, ymax=16000)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"e2e_without.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze script for quota change experiment for e2e and append latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find burst start lcn and wn\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_burst_cut(log_file_path):    \n",
    "    pattern = r\"burst local cut number (\\d+)\"\n",
    "    with open(log_file_path, \"r\") as file:\n",
    "        log_content = file.read()\n",
    "    matches = re.findall(pattern, log_content)\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# CONFIGURE PATH HERE\n",
    "logfile = \"../results/qc/data-0-0.log\"\n",
    "burst_cut_num = extract_burst_cut(logfile)\n",
    "print(f\"Burst cut number: {burst_cut_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Helper function to parse timestamps\n",
    "def parse_timestamp(ts):\n",
    "    return datetime.strptime(ts, \"%H:%M:%S.%f\")\n",
    "\n",
    "# File path\n",
    "log_file = \"../results/qc/order-0.log\"\n",
    "\n",
    "# Regex patterns\n",
    "tput_pattern = r\"\\[real-time tput\\]: (\\d+) ops/sec\"\n",
    "total_tput_pattern = r\"\\[real-time total tput\\]: (\\d+) ops/sec\"\n",
    "timestamp_pattern = r\"(\\d{2}:\\d{2}:\\d{2}\\.\\d{6})\"\n",
    "cut_pattern = rf\"{timestamp_pattern} cut:(\\d+) cut:(\\d+)\"\n",
    "burst_cut_pattern = rf\"cut:(\\d+) cut:0 localCutNum:{burst_cut_num}\"\n",
    "\n",
    "timestamps = []\n",
    "tput_values = []\n",
    "total_tput_timestamps = []\n",
    "total_tput_values = []\n",
    "shard_0_cuts = []\n",
    "shard_1_cuts = []\n",
    "burst_start = []\n",
    "\n",
    "with open(log_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        # Extract throughput\n",
    "        tput_match = re.search(tput_pattern, line)\n",
    "        total_tput_match = re.search(total_tput_pattern, line)\n",
    "        timestamp_match = re.search(timestamp_pattern, line)\n",
    "        \n",
    "        if tput_match and timestamp_match:\n",
    "            tput_values.append(int(tput_match.group(1)))\n",
    "            timestamps.append(parse_timestamp(timestamp_match.group(1)))\n",
    "\n",
    "        if total_tput_match and timestamp_match:\n",
    "            total_tput_values.append(int(total_tput_match.group(1)))\n",
    "            total_tput_timestamps.append(parse_timestamp(timestamp_match.group(1)))\n",
    "\n",
    "        # Extract shard cuts\n",
    "        cut_match = re.search(cut_pattern, line)\n",
    "        if cut_match:\n",
    "            timestamp, cut_0, cut_1 = cut_match.groups()\n",
    "            shard_0_cuts.append((parse_timestamp(timestamp), int(cut_0)))\n",
    "            shard_1_cuts.append((parse_timestamp(timestamp), int(cut_1)))\n",
    "\n",
    "        # Extract burst start\n",
    "        burst_cut_pattern_match = re.search(burst_cut_pattern, line)\n",
    "        if burst_cut_pattern_match and timestamp_match:\n",
    "            burst_start.append(parse_timestamp(timestamp))\n",
    "\n",
    "\n",
    "if not burst_start:\n",
    "    print(\"No burst start found.\")\n",
    "    exit()\n",
    "burst_start_time = burst_start[0]\n",
    "\n",
    "# CONFIGURE ZOOM WINDOW HERE\n",
    "zoom_window_ms = 10000\n",
    "zoom_start = burst_start_time - timedelta(milliseconds=zoom_window_ms)\n",
    "zoom_end = burst_start_time + timedelta(milliseconds=zoom_window_ms)\n",
    "\n",
    "# Convert to relative times\n",
    "min_timestamp = min(timestamps + total_tput_timestamps + [c[0] for c in shard_0_cuts + shard_1_cuts])\n",
    "\n",
    "def to_relative_ms(t):\n",
    "    return (t - min_timestamp).total_seconds() * 1000\n",
    "\n",
    "shard_0_times_rel = [to_relative_ms(t) for t, _ in shard_0_cuts if zoom_start <= t <= zoom_end]\n",
    "shard_1_times_rel = [to_relative_ms(t) for t, _ in shard_1_cuts if zoom_start <= t <= zoom_end]\n",
    "# quota_times_rel = {to_relative_ms(t): v for t, v in quota_changes.items() if zoom_start <= t <= zoom_end}\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot throughput\n",
    "ax.plot([to_relative_ms(t) for t in timestamps], tput_values, label=\"Throughput (ops/sec)\", color=\"blue\")\n",
    "\n",
    "# Plot total throughput\n",
    "ax.plot([to_relative_ms(t) for t in total_tput_timestamps], total_tput_values, label=\"Total Throughput (ops/sec)\", color=\"green\")\n",
    "\n",
    "# # Plot Shard 0 cuts\n",
    "# ax.scatter(shard_0_times_rel, [1 + 0.05 * i for i in range(len(shard_0_times_rel))],\n",
    "#            label=\"Shard 0 Cuts\", color=\"blue\", marker=\"o\", s=40, alpha=0.6)\n",
    "\n",
    "# # Plot Shard 1 cuts\n",
    "# ax.scatter(shard_1_times_rel, [2 + 0.05 * i for i in range(len(shard_1_times_rel))],\n",
    "#            label=\"Shard 1 Cuts\", color=\"orange\", marker=\"o\", s=40, alpha=0.6)\n",
    "\n",
    "# Plot quota changes\n",
    "# for t_rel, (quota_0, quota_1) in quota_times_rel.items():\n",
    "#     ax.axvline(t_rel, color=\"green\", linestyle=\"--\", alpha=0.8)\n",
    "#     ax.text(t_rel, 3, f\"Q0:{quota_0}\\nQ1:{quota_1}\", color=\"green\", fontsize=8, rotation=45)\n",
    "\n",
    "# Highlight burst start\n",
    "burst_start_rel = to_relative_ms(burst_start_time)\n",
    "ax.axvline(burst_start_rel, color=\"red\", linestyle=\":\", alpha=0.8, label=\"Long-term client started\")\n",
    "\n",
    "# Labels and legend\n",
    "ax.set_xlabel(\"Relative Time (ms)\")\n",
    "ax.set_ylabel(\"Event Level\")\n",
    "ax.set_xlim(to_relative_ms(zoom_start), to_relative_ms(zoom_end))\n",
    "ax.set_title(\"Event Timeline around Burst Start\")\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"qc_tput.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute OL load before and after burst\n",
    "len_shard_0_cuts_before = len([cut for cut in shard_0_cuts if cut[0] < burst_start_time])\n",
    "len_shard_1_cuts_before = len([cut for cut in shard_1_cuts if cut[0] < burst_start_time])\n",
    "\n",
    "len_shard_0_cuts_after = len([cut for cut in shard_0_cuts if cut[0] > burst_start_time])\n",
    "len_shard_1_cuts_after = len([cut for cut in shard_1_cuts if cut[0] > burst_start_time])\n",
    "\n",
    "print(f\"Shard 0 cuts before burst: {len_shard_0_cuts_before}\")\n",
    "print(f\"Shard 1 cuts before burst: {len_shard_1_cuts_before}\")\n",
    "print(f\"Shard 0 cuts after burst: {len_shard_0_cuts_after}\")\n",
    "print(f\"Shard 1 cuts after burst: {len_shard_1_cuts_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## analyze latencies for shard 0\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def parse_timestamp(timestamp):\n",
    "    return datetime.strptime(timestamp, \"%H:%M:%S.%f\")\n",
    "\n",
    "def get_lat_ts(path):\n",
    "    file_pattern = path + \"e2e_metrics.csv\"\n",
    "    append_pattern = path + \"append_metrics.csv\"\n",
    "    e2e_latency_values = {}\n",
    "    append_latency_values = {}\n",
    "\n",
    "    for file in glob.glob(append_pattern):\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()[1:]  # Skip the header\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                gsn, latency, timestamp, tput = int(parts[0]), int(parts[1]), parse_timestamp(parts[2]), float(parts[3])\n",
    "                append_latency_values[gsn] = [latency, timestamp]\n",
    "\n",
    "    for file in glob.glob(file_pattern):\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()[1:]  # Skip the header\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                gsn, e2e_latency, delivery_timestamp = int(parts[0]), int(parts[2]), parse_timestamp(parts[4])\n",
    "                e2e_latency_values[gsn] = [e2e_latency, delivery_timestamp]\n",
    "\n",
    "    with open(path + \"client_node7.log\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if \"first append start time\" in line:\n",
    "                burst_start = parse_timestamp(line.split()[-1])\n",
    "                break\n",
    "\n",
    "    return append_latency_values, e2e_latency_values, burst_start\n",
    "\n",
    "# CONFIGURE PATH HERE\n",
    "path = \"../results/qc/\"\n",
    "append_latency_values, e2e_latency_values, burst_start = get_lat_ts(path)\n",
    "print(f\"burst start: {burst_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Append latency plots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "latency_times = []\n",
    "latencies = []\n",
    "for gsn, (latency, timestamp) in append_latency_values.items():\n",
    "    latency_times.append(timestamp)\n",
    "    latencies.append(latency)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'time': latency_times,\n",
    "    'latency': latencies\n",
    "})\n",
    "\n",
    "df = df.sort_values(by='time')\n",
    "\n",
    "# CONFIGURE WINDOW SIZE HERE\n",
    "window_size = 1000  # Set the window size for the moving average\n",
    "df['moving_avg'] = df['latency'].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "min_time = df['time'].min()\n",
    "df['relative_time_ms'] = (df['time'] - min_time).dt.total_seconds() * 1000\n",
    "\n",
    "# CONFIGURE ZOOM IN PERIOD HERE\n",
    "start_time = burst_start - timedelta(milliseconds=10000)  \n",
    "end_time = burst_start + timedelta(milliseconds=10000)  \n",
    "\n",
    "df_zoomed = df[(df['time'] >= start_time) & (df['time'] <= end_time)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_zoomed['relative_time_ms'], df_zoomed['moving_avg'], label=f'Moving Average (window={window_size})', color='blue', linewidth=2)\n",
    "\n",
    "\n",
    "start_time_relative = (burst_start - min_time).total_seconds() * 1000\n",
    "plt.axvline(x=start_time_relative, color='green', linestyle=':', label=f'burst start time')\n",
    "\n",
    "\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Latency (us)')\n",
    "plt.title('Append Latency Over Time with Moving Average')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(ymin=0, ymax=12000)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### E2E Latency plots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "e2e_latency_times = []\n",
    "e2e_latencies = []\n",
    "for gsn, (latency, timestamp) in e2e_latency_values.items():\n",
    "    e2e_latency_times.append(timestamp)\n",
    "    e2e_latencies.append(latency)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'time': e2e_latency_times,\n",
    "    'latency': e2e_latencies\n",
    "})\n",
    "\n",
    "df = df.sort_values(by='time')\n",
    "\n",
    "# CONFIGURE WINDOW SIZE HERE\n",
    "window_size = 1000  # Set the window size for the moving average\n",
    "df['moving_avg'] = df['latency'].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "min_time = df['time'].min()\n",
    "df['relative_time_ms'] = (df['time'] - min_time).dt.total_seconds() * 1000\n",
    "\n",
    "# CONFIGURE ZOOM IN PERIOD HERE\n",
    "start_time = burst_start - timedelta(milliseconds=10000)  \n",
    "end_time = burst_start + timedelta(milliseconds=10000)  \n",
    "\n",
    "df_zoomed = df[(df['time'] >= start_time) & (df['time'] <= end_time)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_zoomed['relative_time_ms'], df_zoomed['moving_avg'], label=f'Moving Average (window={window_size})', color='blue', linewidth=2)\n",
    "\n",
    "\n",
    "start_time_relative = (burst_start - min_time).total_seconds() * 1000\n",
    "plt.axvline(x=start_time_relative, color='green', linestyle=':', label=f'burst start time')\n",
    "\n",
    "\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Latency (us)')\n",
    "plt.title('e2e Latency Over Time with Moving Average')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(ymin=0, ymax=12000)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output.png\", dpi=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
